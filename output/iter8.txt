Profile: I am Chao Wang. I am a Principal Applied Scientist at Microsoft. I am reachable at wangchao@microsoft.com. I am operating at a senior research-to-production tier, aligning with Strategic Leadership and driving Technical Execution with engineering leaders and teams. I am a key collaborator with Ramesh R, Qiongfang Zhang, Bingqian Shen, Bo Lu, Yu Jin, Meng Zhou, and Wanqin Cao. I am functionally focused on translating scientific theory into scalable engineering frameworks and bridging research-heavy initiatives into product-driven engineering cycles. I have expertise in applied science for production systems, cross-functional engineering execution, and AI-assisted developer workflows.

Description: A tight, action-oriented catch-up on what changed around RTC migration tooling and AI-centric code review—plus the concrete follow-ups that landed after your SOX prompt-and-spec session.

Andrew: Hi Chao, today is Wednesday, February 4, 2026.
Ava: After your SOX AI slides, the room’s main follow-up pressure was on “how to keep prompts from drifting” as models change, and how to avoid iterative prompt overfitting turning into brittle, hyper-specific instructions.
Andrew: The next-step that stuck: another session next Friday to go deeper on the counterintuitive prompt techniques, with open questions queued on how to reliably “query” model common sense and keep it stable across upgrades.
Ava: One practical thread you’ll want to close: versioning and rollback expectations for spec-driven workflows—people want checkpoint semantics that feel as safe as dev tooling, not a one-way prompt experiment.

Andrew: On RTC signaling services tooling, the sharpest actionable idea wasn’t the demos—it was the workflow ergonomics: break migrations into modular mini-steps, then review each step, to keep agent context tight and PRs small.
Ava: That same pattern came up again on OS Porter: there’s appetite for a simple extensibility hook after the inner-loop work finishes—teams want to run their own rollout/rollback or validation scripts without waiting for first-party end-to-end automation.
Andrew: The OS Porter team’s immediate sequencing is clear: finish inner-loop automation first, then consider async script hooks for rollout automation if the design stays lightweight.

Ava: In the agent engineering learning series, the PR review tool demo landed as “context first”: it gathers PR diffs plus adjacent code, pulls work-item and discussion context, and then runs review presets like bug-hunt, perf, or security with outputs that can post back into ADO.
Andrew: The operational follow-up: a binary and source drop is planned so others can contribute—extensibility is a design goal, with talk of an extension model and eventually agent-generated plugins on demand.
Ava: The ecosystem asks to watch: handling accelerated PR velocity—deep links, “ready for review” detection, and keeping automated fixes as separate commits so your rollback story stays clean.

Andrew: If you want one concrete move this week: push for that post-inner-loop script hook in OS Porter—small surface area, big leverage—and it matches the same “task list + incremental review” discipline that came up in both migration and PR review.