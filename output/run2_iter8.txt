Profile: I am Chao Wang. I am a Principal Applied Scientist at Microsoft. I am reachable at wangchao@microsoft.com. I am operating at a senior research-to-production tier, aligning with Strategic Leadership and driving technical execution with engineering managers and software engineers. I am a key collaborator with Ramesh R, Qiongfang Zhang, Bingqian Shen, Bo Lu, Yu Jin, Meng Zhou, and Wanqin Cao. I am focused on translating scientific theory into scalable engineering frameworks and bridging research-heavy initiatives into product engineering cycles. I have expertise in applied science for production systems, AI-assisted developer workflows, and cross-functional engineering execution, so I don’t need foundational explanations in those areas.
Description: Action-oriented catch-up across SOX/RTC and agent engineering: what landed, what people pushed back on, and the concrete follow-ups around migration automation and AI-centric PR review velocity.

Ava: Hi Chao, today is Wednesday, February 4, 2026.
Andrew: Your AI slides triggered mostly clarification pressure: multiple people kept probing whether the “ideal” region implied outputs beyond model capacity, and there was repeated confusion about the x-axis meaning in a high-dimensional sampling story.
Ava: Net takeaway from the room: they want the picture grounded in something operational—how to interpret “sampling points” without hand-waving—and how to keep the “not too rigid, not too random” framing stable as models change.

Andrew: Follow-ups that emerged: there’s demand to go deeper on the counterintuitive prompt techniques next Friday, and there’s an open thread on prompt drift—how to keep prompts effective across model upgrades without iterative overfitting.
Ava: Also, the spec-driven workflow angle landed as a practical direction: people latched onto checkpoint/restore and versioning/rollback as the real adoption gate, plus the desire to not be locked to a single model.

Andrew: Tech Talk on RTC signaling services was really two tools and one theme: shrink migration effort, but keep humans in the loop.
Ava: Package Modernizer is being used broadly already—dozens of projects—while still optimizing for “buildable, high-quality code” and pushing manual review to service teams.

Andrew: The sharpest process feedback: someone pushed for smaller, modular task steps with a review after each step to reduce context bloat and avoid giant PRs; the response was agreement and intent to split big changes into smaller commits.
Ava: Another thread: the migration design phase is being treated as a first-class artifact—multiple options from conservative to aggressive—explicitly calling out potential functionality gaps and leaving to-dos when automation can’t close the loop.

Andrew: OS Porter’s reported impact was concrete: partial task coverage today and measurable effort reduction on a real migration, with the boundary that it’s currently strongest at build-error fixing.
Ava: The near-term product center of gravity is the “inner loop”: automated local build, unit tests, and sandbox validation with iterative AI fixes—meant to compress turnaround time.

Andrew: The most actionable ask from the audience was about extensibility: add a simple hook so teams can run their own rollout/rollback or post-inner-loop validations—think a script hook that can call into their pipeline APIs.
Ava: The tool owners didn’t commit, but they acknowledged it as a low-friction lever once the inner loop ships.

Andrew: Agent Engineering learning series: the PR review tool demo was about scaling review, not codegen—pulling context from PRs, work items, and discussion artifacts, then producing walkthroughs and structured review outputs.
Ava: The practical differentiator was workflow hygiene: it stages context into a separate work tree to manage token limits, and it applies trivial fixes as separate commits so rollbacks stay clean.

Andrew: Questions clustered around accuracy and integration: how it avoids missing context, how it chooses between Copilot CLI vs cloud execution, and whether it interferes with local IDEs; the answer was “separate terminal, isolated work tree, selectable backend.”
Ava: The forward-looking pressure was ecosystem-level: extension points so others can add capabilities, deep links, and automation for “ready for review” detection—because review velocity becomes the bottleneck once agents speed up change volume.

Andrew: If you want a tight set of next moves to carry into your week: firm up the graph explanation so the axes interpretation can’t be mistaken, and decide what “model drift” mitigation you’ll recommend as a default practice.
Ava: And on the tooling side, the highest-leverage bet to watch is the inner-loop + hook combo—if that lands, it’s the bridge between “AI helps me migrate/review” and “AI runs my end-to-end loop,” which is where your org will feel the step-change.