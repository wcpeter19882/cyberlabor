Profile: I am Chao Wang. I am a Principal Applied Scientist at Microsoft. I am reachable at wangchao@microsoft.com. I am operating at a senior research-to-production tier, aligning with Partner and Group-level Engineering Directors and executing with Principal Software Engineering Managers and core Software Engineers. I am a key collaborator with Ramesh R, Qiongfang Zhang, Bingqian Shen, Bo Lu, Yu Jin, Meng Zhou, and Wanqin Cao. I am focused on translating scientific theory into scalable engineering frameworks and bridging research-heavy initiatives with product-driven engineering cycles. I have expertise in applied science, research-to-production execution, and cross-functional technical leadership.

Description: Action-oriented catch-up across SOX/RTC and agent tooling: what landed, what got questioned, and the follow-ups that matter—especially around migration automation and AI-centric PR review workflows.

Andrew: Hi Chao, today is Wednesday, February 4, 2026.
Andrew: RTC signaling tooling got a concrete adoption signal: Package Modernizer is already being used at meaningful volume—SS3 Data Platform reported 8 projects, OSI Runtime reported 20—and it’s now in-flight on PSTN Admin service plus Control Plan for .NET Framework to .NET Core moves.
Ava: The sharpest pushback was about workflow shape, not capability. Kiran pressed on agent granularity and suggested breaking migrations into modular mini-steps with code review after each step to avoid context bloat and giant PRs; Wei acknowledged and said they plan to split large PRs into smaller commits.
Andrew: Sainath also probed whether the migration agent actually handles incompatibilities versus just mechanical transforms. The answer was “yes,” with the key mechanism being design proposals that call out gaps and generate explicit human to-dos when it can’t safely close the loop.
Ava: On access, Ali asked the practical question: can teams just use it. The answer was “download the VS Code extension,” but the strong nudge was to engage the team if a service wants real support.

Andrew: OS Porter had the most immediately actionable delta: SS3 Platform’s experience said it covered about 70% of roughly 50 migration tasks, but right now it’s bounded to fixing build errors—so it accelerates the front half, not the full path to prod.
Ava: The tangible metric from that team: 7 PRs on the IC3 Enterprise Entity Runtime Store migration, with about a 28% reduction in development and validation effort.
Andrew: OS Porter’s pipeline is staged, and the near-term bet is the “inner loop”—automated local builds, unit tests, and sandbox validations with iterative AI fixes. That’s the piece they’re trying to finish before expanding further.
Ava: Kiran then steered it toward extensibility: he asked for an “auto loop” idea—simple script hooks after inner loop completion so teams can bolt on rollout/rollback or extra validation via their own automation, like ADO APIs. The OS Porter team didn’t commit, but they were open to exploring asynchronous hook support once inner loop ships.

Andrew: In the SOX RTC bug bash/team meeting, the only thing worth carrying forward for you is what other people pushed on after your slides.
Ava: Gavin questioned whether the “ideal” target region on your sampling graph implies pushing outputs into territory the model can’t naturally support—basically, whether that’s where “fake” answers live—then doubled down with Jun on what the x-axis actually represents in a high-dimensional sampling story.
Andrew: Zhiyuan pushed on the operational question: how do you reliably tap “model common sense” for things like code review criteria without it turning into vibes. Shuo followed with the durability concern—how those prompt behaviors drift when the underlying model changes—so the open thread is prompt maintenance, not prompt cleverness.
Ava: Ming reinforced the iterative-prompting pattern he’s using—AI generates prompts, he refines—and you flagged the overfitting failure mode. Wenlong then asked to go deeper on the counterintuitive techniques next Friday, so expect that follow-up session to be more tactics-heavy.

Andrew: The Agent Engineering learning series was basically a demo of Kiran’s PR review tool and a roadmap pitch for where agent workflows go next.
Ava: Stas Doc’s core move is context assembly: it builds a git worktree for the PR branch, pulls in adjacent code needed to understand call paths, and then hydrates the review with Work IQ notes plus work item context—explicitly to stay under token limits without losing the thread.
Andrew: Review output is designed to land where engineers already live: ADO comments, terminal diffs with context, and a generated walkthrough that orders the PR changes into a narrative flow so reviewers don’t have to reconstruct it.
Ava: The “auto-fix” mode is the accelerant: it can parse review comments, apply trivial fixes as separate commits, and let the user choose between Copilot CLI and Cloud Terminal—keeping reversibility and auditability by isolating changes per commit.
Andrew: The friction points raised were practical. Sainath asked about accuracy and context contamination; Kiran’s answer was multi-source grounding plus selective file forking. Rhett asked about which backend it uses; the answer was user choice.
Ava: Yang raised markdown/rendering reliability, and Kiran’s mitigation was an explicit self-review loop for the model output plus richer diagram rendering. Inderpal asked about file locks across VS and VS Code; Kiran said it runs in its own terminal and shouldn’t lock files.
Andrew: The forward-looking asks converged on extensibility. Tyler proposed an extension model so teams can append functionality; Kiran said the backend is built for that, with a future where agents can generate plugins on demand. Shahab pushed incident investigation agents; Kiran’s blocker there is grounding—turning OneNote and other sources into markdown that tools can actually ingest.
Ava: If you want one concrete follow-up to assign yourself: decide whether you want to engage Kiran on the extension/plugin interface now—while it’s still malleable—because that’s where your research-to-production instincts can prevent a tool-shaped-by-demos from becoming a platform-shaped-by-constraints.