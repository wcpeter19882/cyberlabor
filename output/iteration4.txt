Profile: I am Chao Wang. I am a Principal Applied Scientist at Microsoft. I am reachable at wangchao@microsoft.com. I am operating at a senior research-to-production tier, aligning with Strategic Leadership and driving technical execution with engineering managers and software engineers. I am a key collaborator with Ramesh R, Qiongfang Zhang, Bingqian Shen, Bo Lu, Yu Jin, Meng Zhou, and Wanqin Cao. I am focused on translating scientific theory into scalable engineering frameworks and bridging research-heavy initiatives into product engineering cycles. I have expertise in applied science for production systems, cross-functional technical leadership, and research-to-product execution, so I don’t need foundational explanations in those areas.
Description: Action-oriented takeaways across OS migration tooling and AI-centric dev workflows, plus the specific follow-ups and questions that landed after Chao’s AI slide session in the SOX RTC meeting.

Ava: Hi Chao, today is Wednesday, February 4, 2026—OS migration tooling is converging on the same theme: smaller steps, tighter loops, and explicit hooks for teams to stitch in their own automation.
Andrew: Package Modernizer is running repo-level migrations, but the sharp feedback was about granularity: Kiran pushed for mini-steps plus code review after each step to cut context bloat and avoid giant PRs; Wei Feng said they’re planning to split large PRs into smaller commits.
Ava: On incompatibilities, Sainath pressed whether the agent actually handles them; Wei’s answer landed as “yes, with design proposals plus explicit to-dos,” including cases like conditional compilation and gaps when moving to .NET 8 policies—human follow-up stays part of the contract.
Andrew: Adoption signals were concrete: SS3 Data Platform used it on 8 projects, OSI Runtime on 20, and there’s a component-governance pilot already at scale—100+ merged PRs and 400+ PRs created—so the workflow is moving beyond pure .NET migration into broader upgrade remediation.
Ava: OS Porter’s user story was “70% coverage, but mostly build-error fixing today.” Xiao Chen’s team reported 6 service migrations and, on one service, 7 PRs with about a 28% effort reduction—still with humans validating.
Andrew: The most actionable thread in the OS Porter Q&A was Kiran’s ask: don’t stop at the inner loop—add a simple script hook so teams can run their own rollout/rollback and extra validations, maybe via ADO APIs, after the loop completes.
Ava: Yifan and Yanan didn’t commit to shipping it immediately, but they were receptive: inner loop is the near-term focus, and async hook capability is on the table once the core loop lands.
Andrew: Separate meeting, same direction: in the Agent Engineering learning series, Kiran demoed his PR review tool—Stas Doc—built around “get the right context, then review in modes, then optionally auto-fix as isolated commits.”
Ava: The differentiator is context assembly: it builds a work tree for the PR branch, pulls related code beyond the diff, and ingests Work IQ plus work items and comments into a local review folder—explicitly to stay under token limits while keeping accuracy.
Andrew: Review output isn’t just one channel: it can post ADO comments, generate a walkthrough that orders the code changes logically, and show terminal diffs with context; presets cover bug hunt, style, perf, security, and custom presets can be saved and shared.
Ava: The “auto fix” path is intentionally conservative: trivial fixes become separate commits with SHAs, and there’s a manual review option before committing—aimed at speed without destroying traceability.
Andrew: The forward-looking asks were about extensibility and workflow acceleration: Tyler proposed an extension model where users append functionality; Kiran said the Node backend plus web frontend is designed for it, and longer-term he wants build/deploy automation and incident analysis agents.
Ava: There was also operational realism: questions on context accuracy, markdown rendering errors, and file locks across VS and VS Code—Kiran’s answers centered on isolating the work tree, iterative self-review of outputs, and keeping the tool in its own terminal to avoid locking.
Andrew: Now the SOX RTC meeting where you were active—what mattered was the room’s pushback and what they want next.
Ava: Gavin’s main pressure-test was whether the “ideal” region implies outputs outside the model’s natural capacity; he and Jun also kept drilling on what the axes really mean, pushing for a cleaner mental model of “sampling points” and probability density interpretation.
Andrew: Zhiyuan went straight at the practical edge: how to reliably tap “common sense” without it turning into hand-wavy standards—he wanted a way to know the criteria the model is using, not just assume it.
Ava: Shuo’s concern was durability: prompt instructions that work today can drift with model upgrades, so the implicit request is a maintenance strategy, not just clever prompting.
Andrew: Ming reinforced that the team is actively using “AI to generate prompts, then refine,” but the follow-up tension was the overfitting risk—people want speed without prompts calcifying into brittle, hyper-specific scripts.
Ava: Wenlong’s closing ask was the clearest next step: a deeper follow-up session focused specifically on the counterintuitive prompt techniques, and the group aligned on scheduling it next Friday.
Andrew: If you want one thread to carry into your SOX workflows: the org is converging on “task lists + checkpoints + hooks”—it showed up in migration PR splitting, OS Porter’s proposed automation hook, and Kiran’s PR tool making fixes as revertible commits.
Ava: Near-term actions to watch: whether OS Porter actually adds that post-loop script hook, whether Package Modernizer formalizes small-step PR slicing as default, and whether Kiran ships the Stas Doc binary plus source so teams can start extending it.
Andrew: That’s the delta you’d want for next week’s follow-ups—especially the counterintuitive prompting deep dive and the automation-hook conversation that could unblock rollout at scale.