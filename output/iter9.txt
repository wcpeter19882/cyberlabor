Profile: I am Chao Wang. I am a Principal Applied Scientist at Microsoft. I am reachable at wangchao@microsoft.com. I am operating at a senior research-to-production tier, aligning with Strategic Leadership and driving technical execution with engineering managers and software engineers. I am a collaborator with Ramesh R, Qiongfang Zhang, Bingqian Shen, Bo Lu, Yu Jin, Meng Zhou, and Wanqin Cao. I am focused on applied science that translates scientific theory into scalable engineering frameworks, and I bridge research-heavy initiatives with product-driven engineering cycles. I am an expert in applied science for production systems, cross-functional engineering alignment, and research-to-product execution, so I don’t need foundational explanations in those areas.
Description: Action-oriented catch-up across SOX/RTC: concrete follow-ups from the AI prompt session you led, plus the most actionable deltas from the RTC migration tooling tech talk and the agent-driven PR review tool discussion.

Ava: Hi Chao, today is Wednesday, February 4, 2026.
Ava: From the SOX RTC bug bash / monthly meeting: the room landed on a follow-up session next Friday to go deeper on the “counterintuitive” prompt techniques, with an explicit ask to cover the remaining open questions rather than rehash the slide deck.
Andrew: The most pointed pushback was around stability over time—how to keep prompt instructions effective across model upgrades—so expect that to be a recurring thread; the practical next step is to bring a “drift-handling” pattern people can reuse.
Ava: There was also interest in the double-check / re-check loop as a standard practice, framed as a way to raise confidence via multiple samplings; if you want traction, turn that into a lightweight team habit your org can adopt in reviews and investigations.
Andrew: Tech Talk on RTC signaling services quickly turned into two migration tool updates: Package Modernizer is being positioned around high-quality, buildable code generation with explicit human review gates, and the biggest process delta discussed was splitting oversized PRs into smaller modular steps to reduce agent context bloat.
Ava: The actionable idea that stuck: enforce a task-list-driven migration flow with code review after each mini-step—less “one giant migration PR,” more incremental parity checks—so teams can actually merge continuously instead of batching risk.
Andrew: OS Porter’s near-term roadmap is centered on the “inner loop”—automating local builds, unit tests, and sandbox validation with iterative fixes—while the next obvious extension is a simple script hook so teams can bolt on their own rollout/rollback automation after that loop completes.
Ava: Agent Engineering learning series: the PR review tool demo emphasized harvesting just-enough context (PR diffs plus referenced code, work items, and meeting/design notes) to stay under token limits, then producing outputs reviewers can act on immediately: ADO comments, a walkthrough, and terminal diffs.
Andrew: The workflow piece to watch is auto-fix as separate commits—transparent, revertible changes—plus an extension model so teams can add their own checks; the near-term deliverable is a shared binary and source drop for contributions.
Ava: If you want one clean next action for your week: show up next Friday with a tight “prompt drift + double-check loop” playbook, and tie it to your team’s existing SOX review rhythms so it sticks.