Profile: I am Chao Wang. I am a Principal Applied Scientist at Microsoft. I am reachable at wangchao@microsoft.com. I am operating at a senior research-to-production tier, aligning with Strategic Leadership and driving technical execution with engineering managers and software engineers. I am a key collaborator with Ramesh R, Qiongfang Zhang, Bingqian Shen, Bo Lu, Yu Jin, Meng Zhou, and Wanqin Cao. I am focused on applied science, translating scientific theory into scalable engineering frameworks, and bridging research initiatives into product engineering cycles. I have expertise in applied science-to-engineering translation, cross-functional technical leadership, and productionizing research into engineering frameworks.
Description: Action-oriented catch-up across SOX/RTC and agent-engineering discussions, emphasizing what changed after Chao’s segment, plus the concrete next hooks for migration automation and AI-centric PR review tooling.

Ava: Hi Chao, today is Wednesday, February 4, 2026.
Ava: In the SOX RTC bug bash meeting, the pushback and follow-ups centered on model reliability and prompt drift: the group flagged that iterative prompt refinement can overfit, and there was explicit interest in a deeper follow-up session next Friday focused on the “counterintuitive” prompt techniques.
Andrew: The practical next-step that stuck was to treat prompts more like evolving specs: keep MVP boundaries tight, avoid dragging legacy context forward, and expect to re-baseline prompts when model versions change rather than assuming forward compatibility.
Ava: One concrete workflow thread the room latched onto was checkpointing and rollback in AI-assisted work: the discussion converged on using restore points at “user story” granularity so a bad iteration doesn’t poison the rest of the run—useful if your team is trying to keep long-running automation inside a single session.
Andrew: In the RTC signaling tech talk, the migration tooling conversation turned into “make it chunkable”: feedback was to break large migrations into modular task lists with smaller PRs and stepwise reviews, specifically to reduce agent context bloat and keep review latency down.
Ava: The other actionable idea was extensibility around automation: the ask was for a simple script hook after the inner-loop completes so teams can bolt on rollout/rollback or custom validation without waiting for the tool to implement every pipeline variant.
Andrew: In the agent engineering learning series, the PR review tool demo landed as a full workflow, not just a reviewer: it gathered PR context from multiple sources, generated a walkthrough for reviewers, and could apply trivial fixes as separate commits to keep reversions clean.
Ava: The sharpest follow-up pressure there was ecosystem integration: requests clustered around extension points for custom org workflows, plus mechanisms like deep links and “ready for review” detection to handle higher PR velocity without losing auditability.
Andrew: If you want one immediate move from all three: treat “automation” as two deliverables—your inner loop for fast iteration, and your outer loop as pluggable hooks—so your teams can standardize review and compliance gates without blocking on tool roadmaps.
Ava: Next Friday’s follow-up session is the obvious place to press for specifics: what becomes a reusable spec template, what becomes a blacklist/guardrail library, and what gets versioned so your SOX story doesn’t depend on a single model behaving the same forever.